{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e2339a-4957-4ac7-970c-e9ad1259c337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader\n",
    "\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11cb446c-8e0c-4b96-b5b0-07e8a680f472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents:  ['data\\\\The Lord of the Rings - The Fellowship of the Ring.pdf', 'data\\\\The Lord of the Rings - The Return of the King.pdf', 'data\\\\The Lord of the Rings - The Two Towers.pdf', 'data\\\\The Silmarillion.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading books: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:28<00:00,  7.07s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HF_TOKEN = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# url_loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")\n",
    "#documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "pdf_files = glob.glob('data/*.pdf')\n",
    "print(\"Loading documents: \", pdf_files)\n",
    "all_docs = []\n",
    "\n",
    "for file_path in tqdm(pdf_files, desc=\"Reading books\"):\n",
    "    # Load each document using PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "#data_docs = documents_loader.load()\n",
    "\n",
    "docs = all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e49b951-b8f8-4d9b-a171-618998c744c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting documents: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1805/1805 [00:00<00:00, 2448.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "'Well, er, yes, I suppose so,' stammered Bilbo. 'Where is it?' 'In an envelope, if you must know,' said Bilbo impatiently. 'There on the mantelpiece. Well, no! Here it is in my pocket!' He hesitated. 'Isn't that odd now?' he said softly to himself. 'Yet after all, why not? Why shouldn't it stay there?' Gandalf looked again very hard at Bilbo, and there was a gleam in his eyes. 'I think, Bilbo,' he said quietly, 'I should leave it behind. Don't you want to?' 'Well yes – and no. Now it comes to it, I don't like parting with it at all, I may say. And I don't really see why I should. Why do you want me to?' he asked, and a curious change came over his voice. It was sharp with suspicion and annoyance. 'You are always badgering me about my ring; but you have never bothered me about the other things that I got on my journey.' 'No, but I had to badger you,' said Gandalf. 'I wanted the truth. It was important. Magic rings are – well, magical; and they are rare and curious. I was professionally\n",
      "\n",
      "Chunk 2:\n",
      "they are rare and curious. I was professionally interested in your ring, you may say; and I still am. I should like to know where it is, if you go wandering again. Also I think you have had it quite long enough. You won't need it any more. Bilbo, unless I am quite mistaken.' Bilbo flushed, and there was an angry light in his eyes. His kindly face grew hard. 'Why not?' he cried. 'And what business is it of yours, anyway, to know what I do with my own things? It is my own. I found it. It came to me.' 'Yes, yes,' said Gandalf. 'But there is no need to get angry.' 'If I am it is your fault,' said Bilbo. 'It is mine, I tell you. My own. My precious. Yes, my precious.' The wizard's face remained grave and attentive, and only a flicker in his deep eyes showed that he was startled and indeed alarmed. 'It has been called that before,' he said, 'but not by you.' 'But I say it now. And why not? Even if Gollum said the same once. It's not his now, but mine. And I shall keep it, I say.' Gandalf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Replace multiple newlines or special characters with a space\n",
    "    cleaned = re.sub(r'\\n+', ' ', text)\n",
    "    # Remove any other special characters (optional)\n",
    "    cleaned = re.sub(r'[_\\n]', ' ', cleaned)\n",
    "    # Normalize spaces (replace multiple spaces with a single space)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "\n",
    "all_chunks = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "for doc in tqdm(docs, desc=\"Splitting documents\"):\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "for i, chunk in enumerate(all_chunks[100:102]):\n",
    "    print(f\"Chunk {i + 1}:\\n{chunk.page_content}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d3639a-34e5-4b52-b91c-b0a4efa38c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Pet Projects\\teach-me-this\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15bc8d99-320a-4f6c-bdd4-f92c767392af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Hello I want to see the length of the embeddings for this document.\"\n",
    "print(len(embeddings.embed_documents([query])[0]))\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"lance_database\")\n",
    "table = db.create_table(\n",
    "    \"rag_tmt\",\n",
    "    data=[\n",
    "        {\n",
    "            \"vector\": embeddings.embed_query(\"Hello Computer\"),\n",
    "            \"text\": \"Hello computer!\",\n",
    "            \"id\": \"1\",\n",
    "        }\n",
    "    ],\n",
    "    mode=\"overwrite\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2685196-274d-424f-b75c-158e1bb76feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceDBConnection(D:\\Pet Projects\\teach-me-this\\lance_database)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c450c04d-54fd-43bf-925a-e3d2eb18af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = LanceDB.from_documents(all_chunks, embeddings, connection=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ade231a-7575-4a52-ba23-fa074494d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.lancedb.LanceDB at 0x17fb0befc20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beede9ee-b61b-4e6d-845b-40333929c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema.messages import get_buffer_string\n",
    "\n",
    "def convert_chat_to_prompt(chat_template: ChatPromptTemplate) -> PromptTemplate:\n",
    "    # Format the messages in the chat template without resolving any variables\n",
    "    messages = chat_template.format_messages()\n",
    "    \n",
    "    # Convert the list of messages into a string\n",
    "    message_string = get_buffer_string(messages)\n",
    "    \n",
    "    # Create a new PromptTemplate instance with the message string\n",
    "    prompt_template = PromptTemplate.from_template(message_string)\n",
    "    \n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2903394a-3d73-45f6-b413-ef78311216a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, got ChatPromptTemplate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m\n\u001b[0;32m     18\u001b[0m prompt_value \u001b[38;5;241m=\u001b[39m chat_template\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m     19\u001b[0m     {\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: my_query\n\u001b[0;32m     22\u001b[0m     }\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m prompt_value\n\u001b[1;32m---> 25\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Pet Projects\\teach-me-this\\env\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1107\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_template\u001b[1;34m(cls, template, **kwargs)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\u001b[38;5;28mcls\u001b[39m, template: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a template string.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m    Creates a chat template consisting of a single message assumed to be from\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;124;03m        A new instance of this class.\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1107\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1108\u001b[0m     message \u001b[38;5;241m=\u001b[39m HumanMessagePromptTemplate(prompt\u001b[38;5;241m=\u001b[39mprompt_template)\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_messages([message])\n",
      "File \u001b[1;32mD:\\Pet Projects\\teach-me-this\\env\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:282\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[1;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    250\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptTemplate:\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    *Security warning*:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     _partial_variables \u001b[38;5;241m=\u001b[39m partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n",
      "File \u001b[1;32mD:\\Pet Projects\\teach-me-this\\env\\Lib\\site-packages\\langchain_core\\prompts\\string.py:250\u001b[0m, in \u001b[0;36mget_template_variables\u001b[1;34m(template, template_format)\u001b[0m\n\u001b[0;32m    247\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    249\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 250\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     }\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    253\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m mustache_template_vars(template)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\Lib\\string.py:288\u001b[0m, in \u001b[0;36mFormatter.parse\u001b[1;34m(self, format_string)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, got ChatPromptTemplate"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "    \t(\"system\", \"You are a helpful AI bot. Your name is Jarvis.\"),\n",
    "    \t(\"human\", \"Hello, how are you doing?\"),\n",
    "    \t(\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"I need your help in finding out the answer to the following question using the relevant information added:-\"),\n",
    "    \t(\"human\", \"{user_input}\"),\n",
    "\t]\n",
    ")\n",
    "\n",
    "\n",
    "#my_query = \"What is the name of the pony that Sam Gamgee acquires in Bree after Bill Ferny sells them Bill the Pony?\"\n",
    "my_query = \"What different names does Aragorn have?\"\n",
    "\n",
    "\n",
    "prompt_value = chat_template.invoke(\n",
    "    {\n",
    "        \"name\": \"Sid\",\n",
    "        \"user_input\": my_query\n",
    "    }\n",
    ")\n",
    "prompt_value\n",
    "prompt = ChatPromptTemplate.from_me(chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c437bb64-2ba5-4390-b4b8-89aa41f24f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\The Silmarillion.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Silmarillion.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Lord of the Rings - The Two Towers.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Lord of the Rings - The Two Towers.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Silmarillion.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n",
      "data\\The Lord of the Rings - The Two Towers.pdf\n",
      "data\\The Silmarillion.pdf\n",
      "data\\The Lord of the Rings - The Return of the King.pdf\n"
     ]
    }
   ],
   "source": [
    "#prompt = ChatPromptTemplate.from_template(chat_template)\n",
    "\n",
    "retriever = docsearch.as_retriever(search_kwargs={\"k\": 15})\n",
    "docs = retriever.invoke(my_query)\n",
    "for doc in docs:\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aafa51e0-7881-4869-8425-65dfd3a2139f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'dict' and 'ChatPromptValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[0;32m     11\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 12\u001b[0m         \u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunnablePassthrough\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt_value\u001b[49m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;241m|\u001b[39m model\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m response \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39minvoke(my_query)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'dict' and 'ChatPromptValue'"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Model architecture\n",
    "llm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"\n",
    "model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}\n",
    "model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "        {\"context\": retriever,  \"query\": RunnablePassthrough()}\n",
    "        | prompt_value\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "response = rag_chain.invoke(my_query)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
